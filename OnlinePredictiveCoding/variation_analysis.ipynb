{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Variation Types in Feature Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Comparison of Performance of a Traditional Learner in Different Variation Patterns\n",
    "We try to observe any recurring pattern in terms of performance when variation pattern differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "german varying_gaussian opc_backprop 0.46895\n",
      "ionosphere varying_gaussian opc_backprop 0.5524285714285715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/copperwasp/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-c01f7d2c110d>\", line 40, in <module>\n",
      "    fold_errors, fold_losses, fold_weights, fold_masks = trainer.cross_validation(X, y, m, p.folds, p.scenarios[scenario])\n",
      "  File \"/Users/copperwasp/Desktop/PredictiveCoding/OnlinePredictiveCoding/trainer.py\", line 41, in cross_validation\n",
      "    losses, predictions, error_rate, weights = train(X_copy, y, model)\n",
      "  File \"/Users/copperwasp/Desktop/PredictiveCoding/OnlinePredictiveCoding/trainer.py\", line 12, in train\n",
      "    yhat = model.predict(X[i])\n",
      "  File \"/Users/copperwasp/Desktop/PredictiveCoding/OnlinePredictiveCoding/model.py\", line 148, in predict\n",
      "    return self.forward(x)\n",
      "  File \"/Users/copperwasp/Desktop/PredictiveCoding/OnlinePredictiveCoding/model.py\", line 156, in forward\n",
      "    out += self.w[i](x_current)\n",
      "  File \"/Users/copperwasp/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 729, in _call_impl\n",
      "    if (len(self._backward_hooks) > 0) or (len(_global_backward_hooks) > 0):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/copperwasp/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/copperwasp/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/copperwasp/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/copperwasp/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/copperwasp/anaconda3/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/copperwasp/anaconda3/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/copperwasp/anaconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/copperwasp/anaconda3/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Users/copperwasp/anaconda3/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Users/copperwasp/anaconda3/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/Users/copperwasp/anaconda3/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "import model\n",
    "import dataloader as dl\n",
    "import numpy as np\n",
    "import trainer\n",
    "import parameters as p\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "datasets = ['german', 'ionosphere', 'spambase', 'magic04', 'a8a']\n",
    "model_type = 'opc_backprop'\n",
    "\n",
    "torch.manual_seed(p.random_state)\n",
    "np.random.seed(p.random_state)\n",
    "\n",
    "\n",
    "\n",
    "# initialize results dict\n",
    "results = {}\n",
    "masks = {}\n",
    "occurrences = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    results[dataset] = {}\n",
    "    masks[dataset] = {}\n",
    "    occurrences[dataset] = {}\n",
    "    for scenario in p.scenarios:\n",
    "        results[dataset][scenario] = 0\n",
    "        masks[dataset][scenario] = []\n",
    "\n",
    "for scenario in p.scenarios:\n",
    "    for dataset_name in datasets:\n",
    "        Xpath, ypath = dl.get_path(dataset_name)\n",
    "        X, y = dl.read_dataset(Xpath, ypath)\n",
    "        num_features = len(X[0])\n",
    "        m = p.models[model_type](num_features, p.learning_rate)\n",
    "        fold_errors, fold_losses, fold_weights, fold_masks = trainer.cross_validation(X, y, m, p.folds, p.scenarios[scenario])\n",
    "        masks[dataset_name][scenario] = fold_masks\n",
    "        results[dataset_name][scenario] = np.mean(fold_errors)\n",
    "        print(dataset_name, scenario, model_type, np.mean(fold_errors))\n",
    "    print()    \n",
    "    \n",
    "    \n",
    "# plot results for each dataset\n",
    "for dataset_name in datasets:\n",
    "    plt.title(dataset_name)\n",
    "    plt.ylabel('Error Rate')\n",
    "    plt.bar(results[dataset_name].keys(), results[dataset_name].values())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in masks:\n",
    "    for scenario in p.scenarios:\n",
    "        sum_masks = copy.deepcopy(masks[key][scenario][0])\n",
    "        for i in range(1, len(masks[key][scenario])):\n",
    "            sum_masks += masks[key][scenario][i]\n",
    "\n",
    "        avg_sum_masks =  sum_masks / len(masks[key][scenario])\n",
    "        avg_sum_masks = np.sum(avg_sum_masks, axis=0) / len(sum_masks)\n",
    "        occurrences[key][scenario] = avg_sum_masks\n",
    "        \n",
    "        plt.title(key + scenario)\n",
    "        plt.plot(occurrences[key][scenario])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have made two observations so far:**\n",
    "1. Performance in non-uniformly distributed variation in feature spaces is worse than uniform.\n",
    "2. Checking the average availability frequency of features don't immediately give an explanation about this.\n",
    "\n",
    "After this, Jeev suggested to look at the co-occurrences of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance vs. Feature Co-Occurrence Patterns\n",
    "Let us take the *german* dataset and see how feature co-occurrences look like in different versions of the variation.\n",
    "To do this, we first merge the masks from different folds of cross-validation we have.\n",
    "Masks represent the feature availability in a training instance, therefore, useful when we are working on structures of feature spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cov_mat(masks, dataset, setting):\n",
    "    m = masks[dataset][setting]\n",
    "    joint_mask = m[0]\n",
    "    \n",
    "    for i in range(1, len(m)):\n",
    "        joint_mask = np.vstack((joint_mask, m[i]))\n",
    "    \n",
    "    cov_mat = np.cov(joint_mask.T)\n",
    "    sns.heatmap(cov_mat)\n",
    "    plt.show()\n",
    "    return cov_mat\n",
    "\n",
    "\n",
    "settings = ['full', 'varying_uniform', 'varying_gaussian']\n",
    "\n",
    "for dataset in datasets:\n",
    "    for setting in settings:\n",
    "        print(dataset)\n",
    "        show_cov_mat(masks, dataset, setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that an important is that varying feature spaces distribution in a uniform fashion have low covariance -> feature co-occurrence. This means feature occurrences don't follow a particular pattern. On the other hand, for gaussian, there exists various amounts of covariance between features, starting to form a pattern. This seems to be making learning harder for some reason.\n",
    "\n",
    "**Note:** In both cases, the diagonal of the matrix seems to be 0.25. Diagonals of a covariance matrix are the variances of the components of the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship Between the Amount of Covariance in Varying Gaussian and the Traditional Model Performance\n",
    "In this section, we take the generator of varying_gaussian and modify it in a way that it removes features in different levels of variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variations = [0, 0.25, 0.5, 0.75]\n",
    "for variation in variations:\n",
    "    p.cov_strength = variation\n",
    "    print(\"Cov Strength: %f\" % variation)\n",
    "# initialize results dict\n",
    "    results = {}\n",
    "    masks = {}\n",
    "    occurrences = {}\n",
    "\n",
    "    for dataset in datasets:\n",
    "        results[dataset] = {}\n",
    "        masks[dataset] = {}\n",
    "        occurrences[dataset] = {}\n",
    "        for scenario in p.scenarios:\n",
    "            results[dataset][scenario] = 0\n",
    "            masks[dataset][scenario] = []\n",
    "\n",
    "    for scenario in ['varying_gaussian']:\n",
    "        for dataset_name in p.datasets:\n",
    "            Xpath, ypath = dl.get_path(dataset_name)\n",
    "            X, y = dl.read_dataset(Xpath, ypath)\n",
    "            num_features = len(X[0])\n",
    "            m = p.models[model_type](num_features, p.learning_rate)\n",
    "            fold_errors, fold_losses, fold_weights, fold_masks = trainer.cross_validation(X, y, m, p.folds, p.scenarios[scenario])\n",
    "            masks[dataset_name][scenario] = fold_masks\n",
    "            print('Total features received: %f' % np.sum(fold_masks))\n",
    "            print('Avg. features per instance: %f' % (np.sum(fold_masks) / p.folds / len(fold_masks[0])))\n",
    "            results[dataset_name][scenario] = np.mean(fold_errors)\n",
    "            print(dataset_name, scenario, model_type, np.mean(fold_errors))\n",
    "            print()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Observations\n",
    "1. Jeev's backprop version performs better than mine for some reason -> understand why.\n",
    "2. Jeev's backprop version's performance degrades if local biases are removed.\n",
    "3. Performance won't change too much if the trainable weights in between the layers are removed.\n",
    "4. Performance won't change if weights are initialized to 0 instead of Xavier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
