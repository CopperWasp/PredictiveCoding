{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Variation Types in Feature Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Comparison of Performance of a Traditional Learner in Different Variation Patterns\n",
    "We try to observe any recurring pattern in terms of performance when variation pattern differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'opcbackprop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a8fc97db00c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mypath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mfold_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscenarios\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscenario\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscenario\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfold_masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'opcbackprop'"
     ]
    }
   ],
   "source": [
    "import model\n",
    "import dataloader as dl\n",
    "import numpy as np\n",
    "import trainer\n",
    "import parameters as p\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "datasets = ['german', 'ionosphere', 'spambase', 'magic04', 'a8a']\n",
    "model_type = 'opc_backprop'\n",
    "\n",
    "torch.manual_seed(p.random_state)\n",
    "np.random.seed(p.random_state)\n",
    "\n",
    "\n",
    "\n",
    "# initialize results dict\n",
    "results = {}\n",
    "masks = {}\n",
    "occurrences = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    results[dataset] = {}\n",
    "    masks[dataset] = {}\n",
    "    occurrences[dataset] = {}\n",
    "    for scenario in p.scenarios:\n",
    "        results[dataset][scenario] = 0\n",
    "        masks[dataset][scenario] = []\n",
    "\n",
    "for scenario in p.scenarios:\n",
    "    for dataset_name in datasets:\n",
    "        Xpath, ypath = dl.get_path(dataset_name)\n",
    "        X, y = dl.read_dataset(Xpath, ypath)\n",
    "        num_features = len(X[0])\n",
    "        m = p.models[model_type](num_features, p.learning_rate)\n",
    "        fold_errors, fold_losses, fold_weights, fold_masks = trainer.cross_validation(X, y, m, p.folds, p.scenarios[scenario])\n",
    "        masks[dataset_name][scenario] = fold_masks\n",
    "        results[dataset_name][scenario] = np.mean(fold_errors)\n",
    "        print(dataset_name, scenario, model_type, np.mean(fold_errors))\n",
    "    print()    \n",
    "    \n",
    "    \n",
    "# plot results for each dataset\n",
    "for dataset_name in datasets:\n",
    "    plt.title(dataset_name)\n",
    "    plt.ylabel('Error Rate')\n",
    "    plt.bar(results[dataset_name].keys(), results[dataset_name].values())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in masks:\n",
    "    for scenario in p.scenarios:\n",
    "        sum_masks = copy.deepcopy(masks[key][scenario][0])\n",
    "        for i in range(1, len(masks[key][scenario])):\n",
    "            sum_masks += masks[key][scenario][i]\n",
    "\n",
    "        avg_sum_masks =  sum_masks / len(masks[key][scenario])\n",
    "        avg_sum_masks = np.sum(avg_sum_masks, axis=0) / len(sum_masks)\n",
    "        occurrences[key][scenario] = avg_sum_masks\n",
    "        \n",
    "        plt.title(key + scenario)\n",
    "        plt.plot(occurrences[key][scenario])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have made two observations so far:**\n",
    "1. Performance in non-uniformly distributed variation in feature spaces is worse than uniform.\n",
    "2. Checking the average availability frequency of features don't immediately give an explanation about this.\n",
    "\n",
    "After this, Jeev suggested to look at the co-occurrences of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance vs. Feature Co-Occurrence Patterns\n",
    "Let us take the *german* dataset and see how feature co-occurrences look like in different versions of the variation.\n",
    "To do this, we first merge the masks from different folds of cross-validation we have.\n",
    "Masks represent the feature availability in a training instance, therefore, useful when we are working on structures of feature spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cov_mat(masks, dataset, setting):\n",
    "    m = masks[dataset][setting]\n",
    "    joint_mask = m[0]\n",
    "    \n",
    "    for i in range(1, len(m)):\n",
    "        joint_mask = np.vstack((joint_mask, m[i]))\n",
    "    \n",
    "    cov_mat = np.cov(joint_mask.T)\n",
    "    sns.heatmap(cov_mat)\n",
    "    plt.show()\n",
    "    return cov_mat\n",
    "\n",
    "\n",
    "settings = ['full', 'varying_uniform', 'varying_gaussian']\n",
    "\n",
    "for dataset in datasets:\n",
    "    for setting in settings:\n",
    "        print(dataset)\n",
    "        show_cov_mat(masks, dataset, setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that an important is that varying feature spaces distribution in a uniform fashion have low covariance -> feature co-occurrence. This means feature occurrences don't follow a particular pattern. On the other hand, for gaussian, there exists various amounts of covariance between features, starting to form a pattern. This seems to be making learning harder for some reason.\n",
    "\n",
    "**Note:** In both cases, the diagonal of the matrix seems to be 0.25. Diagonals of a covariance matrix are the variances of the components of the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship Between the Amount of Covariance in Varying Gaussian and the Traditional Model Performance\n",
    "In this section, we take the generator of varying_gaussian and modify it in a way that it removes features in different levels of variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variations = [0, 0.25, 0.5, 0.75]\n",
    "for variation in variations:\n",
    "    p.cov_strength = variation\n",
    "    print(\"Cov Strength: %f\" % variation)\n",
    "# initialize results dict\n",
    "    results = {}\n",
    "    masks = {}\n",
    "    occurrences = {}\n",
    "\n",
    "    for dataset in datasets:\n",
    "        results[dataset] = {}\n",
    "        masks[dataset] = {}\n",
    "        occurrences[dataset] = {}\n",
    "        for scenario in p.scenarios:\n",
    "            results[dataset][scenario] = 0\n",
    "            masks[dataset][scenario] = []\n",
    "\n",
    "    for scenario in ['varying_gaussian']:\n",
    "        for dataset_name in p.datasets:\n",
    "            Xpath, ypath = dl.get_path(dataset_name)\n",
    "            X, y = dl.read_dataset(Xpath, ypath)\n",
    "            num_features = len(X[0])\n",
    "            m = p.models[model_type](num_features, p.learning_rate)\n",
    "            fold_errors, fold_losses, fold_weights, fold_masks = trainer.cross_validation(X, y, m, p.folds, p.scenarios[scenario])\n",
    "            masks[dataset_name][scenario] = fold_masks\n",
    "            print('Total features received: %f' % np.sum(fold_masks))\n",
    "            print('Avg. features per instance: %f' % (np.sum(fold_masks) / p.folds / len(fold_masks[0])))\n",
    "            results[dataset_name][scenario] = np.mean(fold_errors)\n",
    "            print(dataset_name, scenario, model_type, np.mean(fold_errors))\n",
    "            print()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(fold_masks) / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fold_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
